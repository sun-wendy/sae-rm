{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendysun/Desktop/sae-rm/stats/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/wendysun/Desktop/sae-rm/stats/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"gpt2\"\n",
    "model = HookedTransformer.from_pretrained(model_name, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsifiers import SAE\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "layer = 6\n",
    "# e2e_name = \"https://huggingface.co/apollo-research/e2e-saes-gpt2/blob/main/downstream_similar_ce_layer_6.pt\"\n",
    "downstream_name = f\"downstream_similar_ce_layer_{layer}.pt\"\n",
    "# downstream_name = f\"downstream_similar_l0_layer_{layer}.pt\"\n",
    "local_name = f\"local_similar_ce_layer_{layer}.pt\"\n",
    "# local_name = f\"local_similar_l0_layer_{layer}.pt\"\n",
    "\n",
    "activation_name = f\"transformer.h.{layer}\"\n",
    "model_id = \"apollo-research/e2e-saes-gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5w/vscxf8713fs_2w7g5f_rsb4c0000gn/T/ipykernel_27938/719909955.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae_list = torch.load(ae_download_location)\n"
     ]
    }
   ],
   "source": [
    "def load_sae(model_id, hh_file_location):\n",
    "    ae_download_location = hf_hub_download(repo_id=model_id, filename=hh_file_location)\n",
    "\n",
    "    sae_list = torch.load(ae_download_location)\n",
    "\n",
    "    # Rename the keys in the state dictionary\n",
    "    def rename_keys(state_dict):\n",
    "        renamed_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            if key == 'encoder.weight':\n",
    "                renamed_state_dict['encoder.0.weight'] = value\n",
    "            elif key == 'encoder.bias':\n",
    "                renamed_state_dict['encoder.0.bias'] = value\n",
    "            else:\n",
    "                renamed_state_dict[key] = value\n",
    "        return renamed_state_dict\n",
    "\n",
    "    # Create an instance of the SAE model with the extracted parameters\n",
    "    n_dict_components, input_size = sae_list['encoder.weight'].shape\n",
    "    sae = SAE(input_size=input_size, n_dict_components=n_dict_components)\n",
    "\n",
    "    # Rename the keys in the state dictionary\n",
    "    renamed_sae_list = rename_keys(sae_list)\n",
    "\n",
    "    # Load the state dictionary into the model instance\n",
    "    sae.load_state_dict(renamed_sae_list)\n",
    "    return sae\n",
    "\n",
    "sae_local = load_sae(model_id, local_name).to(device)\n",
    "sae_downstream = load_sae(model_id, downstream_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading stas/openwebtext-10k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "def download_dataset(dataset_name, tokenizer, max_length=256, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "        lambda x: tokenizer(x['text']),\n",
    "        batched=True,\n",
    "    ).filter(\n",
    "        lambda x: len(x['input_ids']) > max_length\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "dataset_name = \"stas/openwebtext-10k\"\n",
    "max_seq_length = 40\n",
    "print(f\"Downloading {dataset_name}\")\n",
    "dataset = download_dataset(dataset_name, tokenizer=model.tokenizer, max_length=max_seq_length, num_datapoints=2000) # num_datapoints grabs all of them if None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:55<00:00,  3.44s/it]\n",
      "100%|██████████| 16/16 [00:57<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# now run through the model & grab activations to get dead features\n",
    "def get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32):\n",
    "    device = model.cfg.device\n",
    "    num_features, d_model = autoencoder.encoder[0].weight.shape\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "    token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            batch = batch.to(device)\n",
    "            token_list[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "            # with Trace(model, cache_name) as ret:\n",
    "            #     _ = model(batch).logits\n",
    "            #     internal_activations = ret.output\n",
    "            #     # check if instance tuple\n",
    "            #     if(isinstance(internal_activations, tuple)):\n",
    "            #         internal_activations = internal_activations[0]\n",
    "            _, cache = model.run_with_cache(batch)\n",
    "            # print(cache)\n",
    "            internal_activations = cache[cache_name]\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encoder(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations, token_list\n",
    "\n",
    "batch_size = 128\n",
    "activation_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "nz_indices = []\n",
    "\n",
    "for sae in [sae_local, sae_downstream]:\n",
    "    dictionary_activations, _ = get_dictionary_activations(model, dataset, activation_name, max_seq_length, sae, batch_size=batch_size)\n",
    "    nz_indices.append((dictionary_activations.sum(0) != 0).nonzero()[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_decoder = True\n",
    "\n",
    "if(do_decoder):\n",
    "    dec_local = sae_local.decoder.weight.data\n",
    "    dec_downstream = sae_downstream.decoder.weight.data\n",
    "else:\n",
    "    dec_local = sae_local.encoder[0].weight.data.T\n",
    "    dec_downstream = sae_downstream.encoder[0].weight.data.T\n",
    "\n",
    "dec_local = dec_local / dec_local.norm(dim=0, keepdim=True)\n",
    "dec_downstream = dec_downstream / dec_downstream.norm(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cos sim\n",
    "cos_sim = torch.mm(dec_local.T, dec_downstream)\n",
    "\n",
    "# Find pairs w/ cos sim > 0.999\n",
    "threshold = 0.999\n",
    "high_cos_sim_pairs = torch.nonzero(cos_sim > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_cos_sim_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_activations(sae, dataset, activation_name, feature_idx, batch_size=128):\n",
    "    device = model.cfg.device\n",
    "    activations = []\n",
    "    \n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for batch in tqdm(dl, desc=f\"Processing feature {feature_idx}\"):\n",
    "            batch = batch.to(device)\n",
    "            _, cache = model.run_with_cache(batch)\n",
    "            internal_activations = cache[activation_name]\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\")\n",
    "            feature_activations = sae.encoder(batched_neuron_activations)[:, feature_idx]\n",
    "            activations.append(feature_activations.cpu())\n",
    "    \n",
    "    return torch.cat(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_activation_contexts(activations, token_list, top_k=5):\n",
    "    top_indices = torch.argsort(activations, descending=True)[:top_k]\n",
    "    contexts = [token_list[idx-2:idx+3].tolist() for idx in top_indices]\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature 2372: 100%|██████████| 16/16 [00:55<00:00,  3.44s/it]\n",
      "Processing feature 2372: 100%|██████████| 16/16 [00:51<00:00,  3.24s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:49<00:00,  3.06s/it]\n",
      "Processing feature 3264: 100%|██████████| 16/16 [00:49<00:00,  3.08s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:49<00:00,  3.09s/it]\n",
      "Processing feature 3593: 100%|██████████| 16/16 [00:50<00:00,  3.15s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:48<00:00,  3.04s/it]\n",
      "Processing feature 10541: 100%|██████████| 16/16 [00:48<00:00,  3.01s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:47<00:00,  3.00s/it]\n",
      "Processing feature 10802: 100%|██████████| 16/16 [00:49<00:00,  3.09s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:48<00:00,  3.01s/it]\n",
      "Processing feature 14820: 100%|██████████| 16/16 [00:48<00:00,  3.03s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:48<00:00,  3.02s/it]\n",
      "Processing feature 19603: 100%|██████████| 16/16 [00:48<00:00,  3.06s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:49<00:00,  3.08s/it]\n",
      "Processing feature 22920: 100%|██████████| 16/16 [00:48<00:00,  3.03s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:48<00:00,  3.06s/it]\n",
      "Processing feature 27151: 100%|██████████| 16/16 [00:48<00:00,  3.02s/it]\n",
      "Processing feature 3885: 100%|██████████| 16/16 [00:49<00:00,  3.09s/it]\n",
      "Processing feature 30965: 100%|██████████| 16/16 [00:49<00:00,  3.07s/it]\n",
      "100%|██████████| 10/10 [16:31<00:00, 99.11s/it]\n"
     ]
    }
   ],
   "source": [
    "subset_examples = []\n",
    "\n",
    "for local_idx, downstream_idx in tqdm(high_cos_sim_pairs[:10]):\n",
    "    local_activations = get_feature_activations(sae_local, dataset, activation_name, local_idx)\n",
    "    downstream_activations = get_feature_activations(sae_downstream, dataset, activation_name, downstream_idx)\n",
    "    \n",
    "    # Check if local feature activates on a subset of downstream feature\n",
    "    # Count as active if above mean + std\n",
    "    local_active = local_activations > local_activations.mean() + local_activations.std()\n",
    "    downstream_active = downstream_activations > downstream_activations.mean() + downstream_activations.std()\n",
    "    \n",
    "    if local_active.sum() < downstream_active.sum() and (local_active & downstream_active).sum() / local_active.sum() > 0.2:\n",
    "        local_contexts = find_activation_contexts(local_activations, token_list)\n",
    "        downstream_contexts = find_activation_contexts(downstream_activations, token_list)\n",
    "        \n",
    "        subset_examples.append({\n",
    "            'local_idx': local_idx.item(),\n",
    "            'downstream_idx': downstream_idx.item(),\n",
    "            'local_contexts': local_contexts,\n",
    "            'downstream_contexts': downstream_contexts\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 local feature that is a subset of downstream feature\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(subset_examples)} local feature that is a subset of downstream feature\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
